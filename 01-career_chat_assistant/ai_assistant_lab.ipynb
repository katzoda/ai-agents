{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pushover\n",
    "\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(message):\n",
    "    print(f\"Push: {message}\")\n",
    "    payload = {\"user\": pushover_user, \"token\": pushover_token, \"message\": message}\n",
    "    requests.post(pushover_url, data=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push: HEY!!\n"
     ]
    }
   ],
   "source": [
    "push(\"HEY!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool function for recording user details\n",
    "def record_user_details(email, name=\"Name not provided\", notes=\"not provided\"):\n",
    "    push(f\"Recording interest from {name} with email {email} and notes {notes}\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool function an LLM uses when it doesn't know the answer to a user's question\n",
    "def record_unknown_question(question):\n",
    "    push(f\"Recording {question} asked that I couldn't answer\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this information is going to be sent to OpenAI LLM\n",
    "# we are telling LLM : you have the ability to do this, tell me If you want me to do it for you\n",
    "# it's a packaged way of describing what this function does in JSON format so an LLM can then decide in its response whether or not -->\n",
    "# it wants to actually call this tool\n",
    "\n",
    "# there is a lot of JSON in the training data\n",
    "\n",
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record that a user is interested in being in touch and provided an email address\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The email address of this user\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name, if they provided it\"\n",
    "            }\n",
    "            ,\n",
    "            \"notes\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any additional information about the conversation that's worth recording to give context\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question that couldn't be answered\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": record_user_details_json},\n",
    "        {\"type\": \"function\", \"function\": record_unknown_question_json}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'record_user_details',\n",
       "   'description': 'Use this tool to record that a user is interested in being in touch and provided an email address',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'email': {'type': 'string',\n",
       "      'description': 'The email address of this user'},\n",
       "     'name': {'type': 'string',\n",
       "      'description': \"The user's name, if they provided it\"},\n",
       "     'notes': {'type': 'string',\n",
       "      'description': \"Any additional information about the conversation that's worth recording to give context\"}},\n",
       "    'required': ['email'],\n",
       "    'additionalProperties': False}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'record_unknown_question',\n",
       "   'description': \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'question': {'type': 'string',\n",
       "      'description': \"The question that couldn't be answered\"}},\n",
       "    'required': ['question'],\n",
       "    'additionalProperties': False}}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can take a list of tool calls, and run them\n",
    "# this func will handle what happens when the LLM response \"Yes I do want to run this tool, pls run it and provide me with output\"\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        # tool_calls:\n",
    "        # [ChatCompletionMessageToolCall(id='call_zjWM3jjS7uXpo4MBfCcQgPws', \n",
    "        # function=Function(arguments='{\"question\":\"What is Daniel Katzor\\'s favorite movie genre?\"}', \n",
    "        # name='record_unknown_question'), type='function')]\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # globals gives us a dictionary which I can use to lookup any function which is in the global scope\n",
    "        tool = globals().get(tool_name)\n",
    "        \n",
    "        result = tool(**arguments) if tool else {}\n",
    "        # this is the result:\n",
    "        # {'role': 'tool', 'content': '{\"recorded\": \"ok\"}', 'tool_call_id': 'call_d2i2kADu9NBly2ROXNfFv1Qf'}\n",
    "        results.append({\"role\": \"tool\",\n",
    "                        \"content\": json.dumps(result),\n",
    "                        \"tool_call_id\": tool_call.id}) # id='call_zjWM3jjS7uXpo4MBfCcQgPws'\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the linkedin profile and a summary information provided for an LLM\n",
    "\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "name = \"Your name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the fact that LLM agent can use these tools\n",
    "# in theory this isn't needed because the JSON we used to described the tool already gives this kind of context\n",
    "# it never hurts to be repetitive in prompting, it will increase the probability that the model will generate tokens ->\n",
    "# consistent with my objective, we are biasing the model to output tokens consistent with the objective\n",
    "# it's good to remind ourselves that LLM is something that generates the most likely next token\n",
    "\n",
    "\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    done = False\n",
    "    # for printing:\n",
    "    message_user = message\n",
    "    while not done:\n",
    "\n",
    "        # This is the call to the LLM - see that we pass in the tools json\n",
    "\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "\n",
    "        # this tells us whether the LLM is done or wants to call a tool\n",
    "        # finish_reason='stop', 'tool_calls'\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "\n",
    "        print(f\"Printing 'finish_reason = response.choices[0].finish_reason': \\n {finish_reason}\")\n",
    "        \n",
    "        # If the LLM wants to call a tool, we do that\n",
    "         \n",
    "        if finish_reason==\"tool_calls\":\n",
    "            print(f\"src: chat / printing message typed by a user after finish reason is tool: \\n {message}\")\n",
    "            message = response.choices[0].message\n",
    "            print(f\"src: chat / printing message as response from the LLM after finish reason is tools > message = response.choices[0].message: \\n {message}\")\n",
    "            # now the message should contain: ChatCompletionMessage(content='text returned by LLM',\n",
    "            #                    refusal=None, \n",
    "            #                    role='assistant', \n",
    "            #                    annotations=[], \n",
    "            #                    audio=None, \n",
    "            #                    function_call=None, \n",
    "            #                    tool_calls=None)\n",
    "\n",
    "            # message now represents an object response.choices[0].message, so I can get an attribute: message.audio. message.tool_calls\n",
    "            tool_calls = message.tool_calls\n",
    "            print(f\"From chat function printing 'tool_calls = message.tool_calls' \\n {tool_calls}\")\n",
    "            results = handle_tool_calls(tool_calls)\n",
    "\n",
    "            print(f\"src: chat / results = handle_tool_calls(tool_calls) : \\n {results}\")\n",
    "\n",
    "            # we append messages with the message and the results and then we loop back and call LLM again until it doesn't want to use tool ->\n",
    "            # in that case the else statemnt is executed and done is set to True >> the chat function returns just the content from LLM to the user\n",
    "            # the LLM could want to use the tool for recording the contact details so there's a possibility of using another tool => use of the while loop\n",
    "            # what it wanted to do / it's just a message\n",
    "            messages.append(message)\n",
    "            print(f\"src: chat / print messages after messages.append(message): \\n{messages}\")\n",
    "            reply = response.choices[0].message.content\n",
    "            print(f\"src: chat / Printing message.content: \\n{reply}\")\n",
    "            # the result of doing it\n",
    "            # {'role': 'tool', 'content': '{\"recorded\": \"ok\"}', 'tool_call_id': 'call_d2i2kADu9NBly2ROXNfFv1Qf'}\n",
    "            messages.extend(results)\n",
    "            print(f\"src: chat / print messages after messages.extend(results): \\n{messages}\")\n",
    "        else:\n",
    "            done = True\n",
    "            print(f\"src: chat / printing message typed by a user when tool is not used: \\n {message_user}\")\n",
    "    print(f\"src: chat / Printing message.content from LLM: \\n{response.choices[0].message.content}\")\n",
    "    return response.choices[0].message.content \n",
    "    # what is response.choices[0].message.content? , is it content from role: assistant?\n",
    "    # yes, it's the response to the user\n",
    "    # is it {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options' = None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build in the Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes the reply of the original message\n",
    "# message - original message which the reply was replying to\n",
    "\n",
    "\n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will take again reply and original message + history\n",
    "# it will return one of these objects we defined here: class Evaluation(BaseModel)\n",
    "# it uses a technique called Structured outputs which is a way that I can require an LLM to response in a form of an object like this\n",
    "# it's JSON behind the scenes\n",
    "\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    # now comes the way you pull the API to use structured outputs (.parse)\n",
    "    # we pass model, messages and specify an object we want to be populated -->\n",
    "    # I'm giving it a schema, a pydantic object and say >> I want you to respond with this object\n",
    "    # it's going to actually respond with JSON and the client library is going to take that JSON and use it to populate the object\n",
    "    # so it's an impression that we are getting a code from LLM\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    # so it returns an instance of the Evaluation populated with the response from the LLM\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a certification related to AI?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "\n",
    "    if \"certification\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "\n",
    "        if response.choices[0].finish_reason==\"tool_calls\":\n",
    "                message = response.choices[0].message\n",
    "                tool_calls = message.tool_calls\n",
    "                results = handle_tool_call(tool_calls)\n",
    "                messages.append(message)\n",
    "                messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "    \n",
    "    # evaluator block\n",
    "    reply = response.choices[0].message.content\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(f\"Passed evaluation - returning reply: \\n{reply}\")\n",
    "    else:\n",
    "        print(f\"Failed evaluation - retrying - failed reply: \\n{reply}\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)      \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
